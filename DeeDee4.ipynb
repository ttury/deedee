{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttury/deedee/blob/main/DeeDee4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "MwpOVNab4wZ-",
        "outputId": "20781a3b-5817-44b1-add1-8d833ecee6d5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4c96c321-ff81-42cd-aced-037d91209813\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4c96c321-ff81-42cd-aced-037d91209813\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving transformer.py to transformer.py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13710"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# transformer.py 파일 업로드 필요\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "src = list(files.upload().values())[0]\n",
        "open('transformer.py','wb').write(src)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "zARU2ZNcHiy0",
        "outputId": "e529b873-7e05-402e-ccf5-f3b172ae7e4d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9c01546b4ced>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./drive/MyDrive/packages/transformer.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './drive/MyDrive/packages/transformer.py'"
          ]
        }
      ],
      "source": [
        "open('./drive/MyDrive/packages/transformer.py', 'wb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "HUJDiJGu9Fhq",
        "outputId": "f6739f0c-ae0d-4499-a48c-7b1a1d12ffde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11823\n",
            "Q        0\n",
            "A        0\n",
            "label    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Q            A  label\n",
              "0           12시 땡!   하루가 또 가네요.      0\n",
              "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
              "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "4          PPL 심하네   눈살이 찌푸려지죠.      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3d9de31-8e1a-487d-8839-8282b0992da1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3d9de31-8e1a-487d-8839-8282b0992da1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c3d9de31-8e1a-487d-8839-8282b0992da1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c3d9de31-8e1a-487d-8839-8282b0992da1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "import time\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from transformer import *\n",
        "\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n",
        "train_data = pd.read_csv('ChatBotData.csv')\n",
        "\n",
        "print(len(train_data))\n",
        "print(train_data.isnull().sum())\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VnfyAfG-A4g",
        "outputId": "69a2fa24-4f96-4f86-9503-b41bbf8e7417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['12시 땡 !', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네']\n",
            "['하루가 또 가네요 .', '위로해 드립니다 .', '여행은 언제나 좋죠 .', '여행은 언제나 좋죠 .', '눈살이 찌푸려지죠 .']\n"
          ]
        }
      ],
      "source": [
        "questions = list()\n",
        "for sentence in train_data['Q']:\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = sentence.strip() # 양쪽 공백 제거\n",
        "  questions.append(sentence)\n",
        "\n",
        "answers = list()\n",
        "for sentence in train_data['A']:\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = sentence.strip() # 양쪽 공백 제거\n",
        "  answers.append(sentence)\n",
        "\n",
        "print(questions[:5])\n",
        "print(answers[:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = questions + answers\n",
        "\n",
        "with open('corpus.txt','w',encoding='UTF-8') as f:\n",
        "    for row in corpus:\n",
        "        f.write(row+'\\n')"
      ],
      "metadata": {
        "id": "c2UQK2PfbW1C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(corpus[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgCc07uoFCxD",
        "outputId": "6959d6a3-ec67-4654-8f5b-af2db9d23e8d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12시 땡 !\n",
            "1지망 학교 떨어졌어\n",
            "3박4일 놀러가고 싶다\n",
            "3박4일 정도 놀러가고 싶다\n",
            "PPL 심하네\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWNPzHVoFJlC",
        "outputId": "99c78fc6-3dda-4b6d-a639-b1c414175f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SOS 토큰 번호 : [8178]\n",
            "EOS 토큰 번호 : [8179]\n",
            "단어 집합의 크기 : 8180\n"
          ]
        }
      ],
      "source": [
        "# 서브워드텍스트인코더를 이용해 단어 사전 생성\n",
        "\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    questions + answers, target_vocab_size=2**13\n",
        ")\n",
        "\n",
        "# 단어 사전에 인덱스 2개 추가(디코더의 입력에 사용하는 SOS, EOS 토큰)\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
        "\n",
        "print('SOS 토큰 번호 :', START_TOKEN)\n",
        "print('EOS 토큰 번호 :', END_TOKEN)\n",
        "print('단어 집합의 크기 :', VOCAB_SIZE)\n",
        "# 패딩 토큰 + 8179번 토큰 = 단어 집합의 크기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_subword_corpus(corpus):\n",
        "  subword_corpus = []\n",
        "  for sent in corpus:\n",
        "    encoded_sent = tokenizer.encode(sent)\n",
        "    decoded_sent = []\n",
        "    for index in encoded_sent:\n",
        "      decoded_sent.append(str(tokenizer._id_to_subword(index-1)))\n",
        "    subword_corpus.append(decoded_sent)\n",
        "  return subword_corpus\n",
        "\n",
        "subword_corpus = make_subword_corpus(corpus)"
      ],
      "metadata": {
        "id": "pWYItWOH6_7o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subword_corpus[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mo-p-1KQ84FZ",
        "outputId": "6ad7b2b9-fd85-4b35-a3f6-1f7ebff93508"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['12', '시_', '땡', ' !'],\n",
              " [\"b'1'\", '지', '망', \"b' '\", '학교_', '떨어졌어'],\n",
              " [\"b'3'\", '박', '4일', \"b' '\", '놀러가고_', '싶다'],\n",
              " [\"b'3'\", '박', '4일', \"b' '\", '정도_', '놀러가고_', '싶다'],\n",
              " [\"b'P'\", \"b'P'\", \"b'L'\", \"b' '\", '심하네']]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install glove_python_binary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGKjZIa_0vLi",
        "outputId": "db48c18b-82da-4861-df8c-c5d667029d3c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting glove_python_binary\n",
            "  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.4.1)\n",
            "Installing collected packages: glove-python-binary\n",
            "Successfully installed glove-python-binary-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "corpus = Corpus() \n",
        "\n",
        "# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성\n",
        "corpus.fit(subword_corpus, window=3)\n",
        "glove = Glove(no_components=256, learning_rate=0.05)\n",
        "\n",
        "# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwFckmco0yZ9",
        "outputId": "1fb639dd-b239-430a-c5cb-69247eaafb49"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing 20 training epochs with 4 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(glove.most_similar(\"실패\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loZi-UWDKmqI",
        "outputId": "ea299b99-a25d-46b4-9bf2-ddd5da689c55"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('문제가_', 0.7553508662533543), ('증_', 0.7552565071543065), ('주면_', 0.7536235740697377), ('사람마다_', 0.7526890297180965)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reVd0VDiGzuH",
        "outputId": "bf07b2b3-0d96-419a-e1c5-f11f03046d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 후의 문장 : [5766, 611, 3509, 141, 685, 3747, 849]\n",
            "기존 문장 : 가스비 비싼데 감기 걸리겠어\n",
            "5766 ----> 가스\n",
            "611 ----> 비 \n",
            "3509 ----> 비싼\n",
            "141 ----> 데 \n",
            "685 ----> 감기 \n",
            "3747 ----> 걸리\n",
            "849 ----> 겠어\n"
          ]
        }
      ],
      "source": [
        "# 서브워드텍스트인코더 토크나이저의 encode()를 사용하여 정수 인코딩\n",
        "\n",
        "sample_string = questions[20]\n",
        "\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print('정수 인코딩 후의 문장 : {}'.format(tokenized_string))\n",
        "\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print('기존 문장 : {}'.format(original_string))\n",
        "\n",
        "for token in tokenized_string:\n",
        "  print ('{} ----> {}'.format(token, tokenizer.decode([token])))\n",
        "  # decode의 경우 리스트를 인자로 받는다\n",
        "  # 한 어절이 정수 인코딩 후에는 여러 서브워드로 맵핑될 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(glove)\n",
        "# glove.dictionary\n",
        "len(glove.word_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WslPpab1lp9",
        "outputId": "7255eaa7-8799-4613-ee0b-98b65eeac400"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8024"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "word2vec = Word2Vec(sentences = subword_corpus, size = 256, window = 3, min_count = 0, workers = 4, sg = 0)\n",
        "word2vec.save('word2vec2') # 모델 저장\n",
        "loaded_word2vec = Word2Vec.load(\"word2vec2\") # 모델 로드"
      ],
      "metadata": {
        "id": "3t7VErtdP-Yy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "\n",
        "print(len(glove.word_vectors[0]))\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, 256))\n",
        "\n",
        "def get_vector(word):\n",
        "  if word in glove.dictionary:\n",
        "    index = glove.dictionary[word]\n",
        "    return glove.word_vectors[index]\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "for index in range(VOCAB_SIZE - 3):\n",
        "  subword = tokenizer._id_to_subword(index)\n",
        "  temp = get_vector(subword)\n",
        "  if temp is not None:\n",
        "    embedding_matrix[index] = temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6LiqrIM6P-S",
        "outputId": "796aa086-f57b-4d4c-eb03-66cd01a9f028"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bD_o-z4CQCV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataframe = pd.DataFrame(embedding_matrix)\n",
        "dataframe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "B4UH4rDpH6lw",
        "outputId": "60dbb2db-fb26-48e5-9bdd-02ac0c88c259"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0     0.374902 -0.220930  0.346664  0.190356 -0.135725 -0.191327  0.130512   \n",
              "1     0.078889  0.128243  0.020384 -0.002038 -0.093152 -0.277866  0.020054   \n",
              "2     0.097408  0.193976 -0.154381 -0.130508  0.162438 -0.133396  0.090741   \n",
              "3     0.072457  0.380852 -0.242994 -0.105706  0.210669 -0.075455  0.110732   \n",
              "4     0.034402 -0.071904 -0.000665 -0.071989 -0.006545 -0.089077  0.012757   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "8175  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "8176  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "8177  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "8178  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "8179  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "\n",
              "           7         8         9    ...       246       247       248  \\\n",
              "0     0.222599  0.277908 -0.103534  ... -0.423148 -0.520842 -0.092600   \n",
              "1     0.275233  0.232757  0.139142  ... -0.240255 -0.105939  0.138204   \n",
              "2     0.152188  0.113723  0.083148  ... -0.197661 -0.125690  0.033860   \n",
              "3     0.169382  0.027043 -0.031713  ... -0.148186 -0.037606 -0.058978   \n",
              "4     0.043041  0.036165  0.077681  ... -0.123474 -0.124995  0.061137   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "8175  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
              "8176  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
              "8177  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
              "8178  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
              "8179  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
              "\n",
              "           249       250       251       252       253       254       255  \n",
              "0     0.243741  0.292972 -0.388110 -0.276810 -0.359022 -0.155439  0.091506  \n",
              "1     0.158927  0.164136 -0.231693 -0.209784 -0.109231 -0.015964 -0.047505  \n",
              "2     0.030857  0.167280 -0.059977 -0.136021 -0.226841  0.064469 -0.044220  \n",
              "3     0.005882  0.154388 -0.048058 -0.102507 -0.197358  0.230151 -0.088693  \n",
              "4     0.073845  0.069021 -0.035103 -0.004320 -0.084238 -0.075939  0.016717  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "8175  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "8176  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "8177  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "8178  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "8179  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "\n",
              "[8180 rows x 256 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ad71bf7e-c89c-46ac-b70f-da9e7ded3feb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.374902</td>\n",
              "      <td>-0.220930</td>\n",
              "      <td>0.346664</td>\n",
              "      <td>0.190356</td>\n",
              "      <td>-0.135725</td>\n",
              "      <td>-0.191327</td>\n",
              "      <td>0.130512</td>\n",
              "      <td>0.222599</td>\n",
              "      <td>0.277908</td>\n",
              "      <td>-0.103534</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.423148</td>\n",
              "      <td>-0.520842</td>\n",
              "      <td>-0.092600</td>\n",
              "      <td>0.243741</td>\n",
              "      <td>0.292972</td>\n",
              "      <td>-0.388110</td>\n",
              "      <td>-0.276810</td>\n",
              "      <td>-0.359022</td>\n",
              "      <td>-0.155439</td>\n",
              "      <td>0.091506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.078889</td>\n",
              "      <td>0.128243</td>\n",
              "      <td>0.020384</td>\n",
              "      <td>-0.002038</td>\n",
              "      <td>-0.093152</td>\n",
              "      <td>-0.277866</td>\n",
              "      <td>0.020054</td>\n",
              "      <td>0.275233</td>\n",
              "      <td>0.232757</td>\n",
              "      <td>0.139142</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.240255</td>\n",
              "      <td>-0.105939</td>\n",
              "      <td>0.138204</td>\n",
              "      <td>0.158927</td>\n",
              "      <td>0.164136</td>\n",
              "      <td>-0.231693</td>\n",
              "      <td>-0.209784</td>\n",
              "      <td>-0.109231</td>\n",
              "      <td>-0.015964</td>\n",
              "      <td>-0.047505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.097408</td>\n",
              "      <td>0.193976</td>\n",
              "      <td>-0.154381</td>\n",
              "      <td>-0.130508</td>\n",
              "      <td>0.162438</td>\n",
              "      <td>-0.133396</td>\n",
              "      <td>0.090741</td>\n",
              "      <td>0.152188</td>\n",
              "      <td>0.113723</td>\n",
              "      <td>0.083148</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.197661</td>\n",
              "      <td>-0.125690</td>\n",
              "      <td>0.033860</td>\n",
              "      <td>0.030857</td>\n",
              "      <td>0.167280</td>\n",
              "      <td>-0.059977</td>\n",
              "      <td>-0.136021</td>\n",
              "      <td>-0.226841</td>\n",
              "      <td>0.064469</td>\n",
              "      <td>-0.044220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.072457</td>\n",
              "      <td>0.380852</td>\n",
              "      <td>-0.242994</td>\n",
              "      <td>-0.105706</td>\n",
              "      <td>0.210669</td>\n",
              "      <td>-0.075455</td>\n",
              "      <td>0.110732</td>\n",
              "      <td>0.169382</td>\n",
              "      <td>0.027043</td>\n",
              "      <td>-0.031713</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.148186</td>\n",
              "      <td>-0.037606</td>\n",
              "      <td>-0.058978</td>\n",
              "      <td>0.005882</td>\n",
              "      <td>0.154388</td>\n",
              "      <td>-0.048058</td>\n",
              "      <td>-0.102507</td>\n",
              "      <td>-0.197358</td>\n",
              "      <td>0.230151</td>\n",
              "      <td>-0.088693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.034402</td>\n",
              "      <td>-0.071904</td>\n",
              "      <td>-0.000665</td>\n",
              "      <td>-0.071989</td>\n",
              "      <td>-0.006545</td>\n",
              "      <td>-0.089077</td>\n",
              "      <td>0.012757</td>\n",
              "      <td>0.043041</td>\n",
              "      <td>0.036165</td>\n",
              "      <td>0.077681</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.123474</td>\n",
              "      <td>-0.124995</td>\n",
              "      <td>0.061137</td>\n",
              "      <td>0.073845</td>\n",
              "      <td>0.069021</td>\n",
              "      <td>-0.035103</td>\n",
              "      <td>-0.004320</td>\n",
              "      <td>-0.084238</td>\n",
              "      <td>-0.075939</td>\n",
              "      <td>0.016717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8175</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8176</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8177</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8178</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8179</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8180 rows × 256 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad71bf7e-c89c-46ac-b70f-da9e7ded3feb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ad71bf7e-c89c-46ac-b70f-da9e7ded3feb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ad71bf7e-c89c-46ac-b70f-da9e7ded3feb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OJO3QYk8HZjc"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 40\n",
        "\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "  for (sent1, sent2) in zip(inputs, outputs):\n",
        "    sent1 = START_TOKEN + tokenizer.encode(sent1) + END_TOKEN\n",
        "    sent2 = START_TOKEN + tokenizer.encode(sent2) + END_TOKEN\n",
        "\n",
        "    tokenized_inputs.append(sent1)\n",
        "    tokenized_outputs.append(sent2)\n",
        "  \n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post') # 뒤를 패딩\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  \n",
        "  return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "questions, answers = tokenize_and_filter(questions, answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0si8SKKKROz",
        "outputId": "73c581bf-161b-436d-c4d4-1cc0b386660f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q 데이터의 크기(shape) : (11823, 40)\n",
            "A 데이터의 크기(shape) : (11823, 40)\n",
            "\n",
            "[8178 7915 4207 3060   41 8179    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "[8178 3844   74 7894    1 8179    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "print('Q 데이터의 크기(shape) :', questions.shape)\n",
        "print('A 데이터의 크기(shape) :', answers.shape)\n",
        "print()\n",
        "print(questions[0])\n",
        "print(answers[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOu_n2fXKcn7",
        "outputId": "f87c0bc5-5446-4c6d-db16-247898486809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8178 3844   74 7894    1 8179    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "[[8178 3844   74 7894    1 8179    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[3844   74 7894    1 8179    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]]\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs' : questions,\n",
        "        'dec_inputs' : answers[:, :-1], # 디코더 입력, 마지막 패딩 토큰 제거\n",
        "    },\n",
        "    {\n",
        "        'outputs' : answers[:, 1:] # 디코더 실제값(교사 강요), 시작 토큰 제거\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "# 전처리 시간을 줄여줌\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "# 데이터를 버퍼에 넣고 무작위로 빼내며 셔플, buffer_size가 데이터 크기보다 커야 함\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "# 데이터를 batch_size 개수대로 분리\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "# prefetch : 학습중일때, 데이터 로드시간을 줄이기 위해 메모리에 미리 적재시킴\n",
        "# AUTOTUNE : 네트워크가 알아서 적재시킬 양을 설정해라\n",
        "\n",
        "print(answers[0]) # 기존 샘플\n",
        "print(answers[:1][:, :-1]) # 디코더 입력 샘플\n",
        "print(answers[:1][:, 1:]) # 디코더 정답 샘플"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Hy67t4vzMqoq"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# 하이퍼파라미터\n",
        "D_MODEL = 256\n",
        "NUM_LAYERS = 2\n",
        "NUM_HEADS = 8\n",
        "DFF = 512\n",
        "DROPOUT = 0.1\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size = VOCAB_SIZE,\n",
        "    num_layers = NUM_LAYERS,\n",
        "    dff = DFF,\n",
        "    d_model = D_MODEL,\n",
        "    num_heads = NUM_HEADS,\n",
        "    dropout = DROPOUT,\n",
        "    embedding_matrix = [embedding_matrix])\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y, y_hat): # metrics에 적용할 함수\n",
        "  y = tf.reshape(y, shape=(-1, MAX_LENGTH - 1))\n",
        "  # -1은 None과 같은 이유, 시작 토큰을 제거했으므로 길이 1 감소\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y, y_hat)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(embedding_matrix[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnWr4ExlN_eD",
        "outputId": "aae613d1-abe9-4de7-d719-df76d5e01989"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xehTjVD6bZ_h",
        "outputId": "acf7eb8a-85af-4e8b-89ac-f87e791e364e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9e0c7698d0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "model.load_weights('./drive/MyDrive/models/DeeDee2_weights')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GLCGfEZYjR0",
        "outputId": "bbc178a6-b74b-43a7-98fa-67b0698c86f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "185/185 [==============================] - 19s 54ms/step - loss: 1.4458 - accuracy: 0.0327\n",
            "Epoch 2/50\n",
            "185/185 [==============================] - 10s 54ms/step - loss: 1.1758 - accuracy: 0.0494\n",
            "Epoch 3/50\n",
            "185/185 [==============================] - 10s 54ms/step - loss: 1.0037 - accuracy: 0.0506\n",
            "Epoch 4/50\n",
            "185/185 [==============================] - 10s 54ms/step - loss: 0.9215 - accuracy: 0.0548\n",
            "Epoch 5/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.8592 - accuracy: 0.0587\n",
            "Epoch 6/50\n",
            "185/185 [==============================] - 10s 55ms/step - loss: 0.7936 - accuracy: 0.0646\n",
            "Epoch 7/50\n",
            "185/185 [==============================] - 10s 55ms/step - loss: 0.7229 - accuracy: 0.0724\n",
            "Epoch 8/50\n",
            "185/185 [==============================] - 10s 55ms/step - loss: 0.6485 - accuracy: 0.0799\n",
            "Epoch 9/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.5695 - accuracy: 0.0884\n",
            "Epoch 10/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.4881 - accuracy: 0.0975\n",
            "Epoch 11/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.4064 - accuracy: 0.1074\n",
            "Epoch 12/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.3283 - accuracy: 0.1180\n",
            "Epoch 13/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.2559 - accuracy: 0.1291\n",
            "Epoch 14/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.1945 - accuracy: 0.1384\n",
            "Epoch 15/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.1446 - accuracy: 0.1466\n",
            "Epoch 16/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.1061 - accuracy: 0.1534\n",
            "Epoch 17/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0802 - accuracy: 0.1582\n",
            "Epoch 18/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0633 - accuracy: 0.1613\n",
            "Epoch 19/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0535 - accuracy: 0.1627\n",
            "Epoch 20/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0496 - accuracy: 0.1632\n",
            "Epoch 21/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0463 - accuracy: 0.1637\n",
            "Epoch 22/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0438 - accuracy: 0.1642\n",
            "Epoch 23/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0394 - accuracy: 0.1650\n",
            "Epoch 24/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0335 - accuracy: 0.1666\n",
            "Epoch 25/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0303 - accuracy: 0.1673\n",
            "Epoch 26/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0257 - accuracy: 0.1685\n",
            "Epoch 27/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0239 - accuracy: 0.1690\n",
            "Epoch 28/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0204 - accuracy: 0.1699\n",
            "Epoch 29/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0194 - accuracy: 0.1701\n",
            "Epoch 30/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0177 - accuracy: 0.1706\n",
            "Epoch 31/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0159 - accuracy: 0.1710\n",
            "Epoch 32/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0150 - accuracy: 0.1714\n",
            "Epoch 33/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0136 - accuracy: 0.1717\n",
            "Epoch 34/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0132 - accuracy: 0.1718\n",
            "Epoch 35/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0116 - accuracy: 0.1722\n",
            "Epoch 36/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0114 - accuracy: 0.1722\n",
            "Epoch 37/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0108 - accuracy: 0.1723\n",
            "Epoch 38/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0103 - accuracy: 0.1726\n",
            "Epoch 39/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0098 - accuracy: 0.1727\n",
            "Epoch 40/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0087 - accuracy: 0.1730\n",
            "Epoch 41/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0086 - accuracy: 0.1730\n",
            "Epoch 42/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0084 - accuracy: 0.1730\n",
            "Epoch 43/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0078 - accuracy: 0.1731\n",
            "Epoch 44/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0079 - accuracy: 0.1732\n",
            "Epoch 45/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0069 - accuracy: 0.1734\n",
            "Epoch 46/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0069 - accuracy: 0.1735\n",
            "Epoch 47/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0067 - accuracy: 0.1735\n",
            "Epoch 48/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0068 - accuracy: 0.1734\n",
            "Epoch 49/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0064 - accuracy: 0.1735\n",
            "Epoch 50/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0061 - accuracy: 0.1736\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 50\n",
        "history = model.fit(dataset, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "IVdpnTk0ZL4L"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  sentence = tf.expand_dims(\n",
        "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "  # 첫번째 차원 추가\n",
        "  \n",
        "  output = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "  for i in range(MAX_LENGTH):\n",
        "    predictions = model(inputs=[sentence, output], training=False)\n",
        "\n",
        "    predictions = predictions[: :, -1:]\n",
        "    # 문제 발생\n",
        "    # 해결 : 학습 자료와 다르게 단어 벡터가 세번째 축에 위치했었음\n",
        "    # 슬라이싱을 세번째 축으로 함으로서 concat 연산이 올바르게 수행되게 함\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "      break\n",
        "    \n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "IPSoYYoYKloD"
      },
      "outputs": [],
      "source": [
        "def predict(sentence):\n",
        "  prediction = evaluate(sentence)\n",
        "\n",
        "  predicted_sentence = tokenizer.decode(\n",
        "      [token for token in prediction if token < tokenizer.vocab_size])\n",
        "  \n",
        "  print('Input : {}'.format(sentence))\n",
        "  print('Output : {}'.format(predicted_sentence))\n",
        "\n",
        "  return predicted_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "qHfecR-CK_Lo"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X92IwUAILL1l"
      },
      "outputs": [],
      "source": [
        "while(True):\n",
        "  sentence = input()\n",
        "  if sentence == \"exit\":\n",
        "    break\n",
        "  output = predict(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Ke0CM9cqQ1lX"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.save_weights('./drive/MyDrive/models/DeeDee4_weights')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vQki85-ERgho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history.keys['acc'])"
      ],
      "metadata": {
        "id": "qXjoEP3XWzLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZyIjqhgoWyIn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DeeDee4.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1k1Qf4IVXQjVPZ9od1e6-Bqcblhz2f1rJ",
      "authorship_tag": "ABX9TyPvZ8NiAmLKN6dbhcy0/OSz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}